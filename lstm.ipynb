{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bWvbig7ifTjnPXrOn016PWjIYMDaPoTd",
      "authorship_tag": "ABX9TyMQEq6+dewYbM1fzVrSx5RD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanocostantini/nlp/blob/main/lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVL8GxeBUnsl"
      },
      "source": [
        "## LSTM model in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sgJu_n9q9Uk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "59d1e404-d15e-418d-f29c-461124cb3506"
      },
      "source": [
        "# Install and load components\n",
        "!pip install tokenizers\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ee/fedc3509145ad60fe5b418783f4a4c1b5462a4f0e8c7bbdbda52bdcda486/tokenizers-0.8.1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.8.1\n",
            "--2020-09-30 15:34:48--  https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.45.238\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.45.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231508 (226K) [text/plain]\n",
            "Saving to: ‘bert-base-uncased-vocab.txt’\n",
            "\n",
            "bert-base-uncased-v 100%[===================>] 226.08K  1.15MB/s    in 0.2s    \n",
            "\n",
            "2020-09-30 15:34:48 (1.15 MB/s) - ‘bert-base-uncased-vocab.txt’ saved [231508/231508]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgFCYiXQUtTJ"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import nltk \n",
        "import re\n",
        "import string\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdAPssDFQbfN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "397c636a-95a8-406e-a1ad-c8b9ef003b09"
      },
      "source": [
        "# Use GPU when present\n",
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "          else torch.device('cpu'))\n",
        "print(f'Training on device: {device}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txv21Bq2gT5A"
      },
      "source": [
        "### Define Dataset class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-s71vOagrK9"
      },
      "source": [
        "We will consider the whole document as a continuous string of text, from which we will form the sequences\n",
        "\n",
        "These will be of fixed length, e.g. 50+1 words, where words 1-50 are used as inputs and 51 is used as ground truth to train the model on\n",
        "\n",
        "Ignoring punctuation, verse numbers etc. we would have:\n",
        "\n",
        "`\"In the beginning God created the heaven and the earth And the earth was without form, and void; and darkness was upon the face of the deep And the Spirit of God moved upon the face of the waters\"`\n",
        "\n",
        "which would then become (assuming a length of 6+1):\n",
        " \n",
        "`\"In the beginning God created the heaven\"`\n",
        "\n",
        "`\"the beginning God created the heaven and\"`\n",
        "\n",
        "`\"beginning God created the heaven and the\"`\n",
        "\n",
        "`\"God created the heaven and the earth\"`\n",
        "\n",
        "And so on... The last word of each of these sequence is what the model will learn to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6pciUWlgg4I"
      },
      "source": [
        "class BibleText(Dataset):\n",
        "  \"\"\"\n",
        "  This class requires an initialised tokenizer which provides the method tokenizer.id\n",
        "  to obtain the word ids. It also requires the following packages:\n",
        "  - re\n",
        "  - string\n",
        "  - numpy as np\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, raw_text, sequence_length, tokenizer):\n",
        "    self.raw_text = raw_text\n",
        "    self.sequence_length = sequence_length\n",
        "    self.tokenizer = tokenizer # using a pre-trained initialized tokenizer\n",
        "    self._init_dataset()\n",
        "  \n",
        "  def _init_dataset(self):\n",
        "    # clean text\n",
        "    clean_text = self.clean_text(self.raw_text)\n",
        "    # apply tokenizer to clean text to encode it\n",
        "    output = self.tokenizer.encode(clean_text)\n",
        "    # extract single sequence of word ids, as generated by the tokenizer\n",
        "    word_ids = output.ids\n",
        "    # generate the sequences (length = sequence_length + 1)\n",
        "    sequences = self.build_sequences(word_ids, self.sequence_length)\n",
        "    # split inputs and targets\n",
        "    sequences_array = np.array(sequences)\n",
        "    inputs_lists = sequences_array[:,:-1]\n",
        "    self.targets = sequences_array[:,-1]\n",
        "    # covert inputs into tensors\n",
        "    self.inputs = torch.tensor(inputs_lists) # len(sequences) * sequence_length\n",
        "\n",
        "  @staticmethod\n",
        "  def clean_text(text):\n",
        "    \"\"\"\n",
        "    Removes line breaks, punctuation and verse numbers. \n",
        "    Returns text in lower case as a single stream of text\n",
        "    \"\"\"\n",
        "    # Remove line breaks\n",
        "    doc = text.replace('\\n', ' ')\n",
        "    # Remove verses numbers\n",
        "    doc_no_verses = re.sub(r\"[0-9]:[0-9]+\", \" \", doc)\n",
        "    # Remove any spaces >= 2\n",
        "    doc_no_spaces = re.sub(r\"[ \\t]{2,}\", \" \", doc_no_verses)\n",
        "    # Removes punctuation\n",
        "    doc_no_punct = doc_no_spaces.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Lower case\n",
        "  \n",
        "    return doc_no_punct.lower()\n",
        "\n",
        "  @staticmethod\n",
        "  def build_sequences(ids, sequence_length):\n",
        "    \"\"\"\n",
        "    Use the ids provided by the tokenizer to build sequences of the desired length\n",
        "    Returns list of sequences of desidered length + 1 (target word)\n",
        "    \"\"\"\n",
        "    length = sequence_length + 1 # add target token at the end\n",
        "    sequences = list()\n",
        "\n",
        "    for id in range(length, len(ids)):\n",
        "      selected_ids = ids[id-length: id] # select the ids\n",
        "      sequences.append(selected_ids)\n",
        "    \n",
        "    return sequences\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.inputs)\n",
        "\n",
        "  def __getitem__(self, id):\n",
        "    \"\"\"\n",
        "    returns a tuple with:\n",
        "    - tensor with input sequence (1 x sequence_length)\n",
        "    - target word id\n",
        "    \"\"\"\n",
        "    return self.inputs[id], self.targets[id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m36qIFXchO_X"
      },
      "source": [
        "# We also define a helper function to convert a list of ids in a both a\n",
        "# list of tokens and also the complete sentence\n",
        "def ids_to_tokens(ids_list, vocab):\n",
        "  \"\"\"\n",
        "  Given a list of ids and the vocabulary they come from\n",
        "  Returns the corresponding tokens, both as list and a string\n",
        "  \"\"\"\n",
        "  tokens = list() # initialise token list\n",
        "  for id in ids_list:\n",
        "    # find token\n",
        "    token = next(key for key, value in vocab.items() if value == id)\n",
        "    # add it to list\n",
        "    tokens.append(token)\n",
        "  \n",
        "  # Join in a single string\n",
        "  string = ' '.join(tokens)\n",
        "\n",
        "  return tokens, string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ilzbLqazLRn"
      },
      "source": [
        "### Load raw text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APFIM93Fm5tA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b223bcba-1680-40ec-994b-f4fdcf89553f"
      },
      "source": [
        "# Load raw text\n",
        "nltk.download('gutenberg')\n",
        "data_raw = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
        "type(data_raw)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzOaQLtWq1jB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "2b1206d2-96df-418b-cdbe-3d22cad53e0e"
      },
      "source": [
        "data_raw[50010:52001]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hagar bare, Ishmael.\\n\\n16:16 And Abram was fourscore and six years old, when Hagar bare\\nIshmael to Abram.\\n\\n17:1 And when Abram was ninety years old and nine, the LORD appeared\\nto Abram, and said unto him, I am the Almighty God; walk before me,\\nand be thou perfect.\\n\\n17:2 And I will make my covenant between me and thee, and will\\nmultiply thee exceedingly.\\n\\n17:3 And Abram fell on his face: and God talked with him, saying, 17:4\\nAs for me, behold, my covenant is with thee, and thou shalt be a\\nfather of many nations.\\n\\n17:5 Neither shall thy name any more be called Abram, but thy name\\nshall be Abraham; for a father of many nations have I made thee.\\n\\n17:6 And I will make thee exceeding fruitful, and I will make nations\\nof thee, and kings shall come out of thee.\\n\\n17:7 And I will establish my covenant between me and thee and thy seed\\nafter thee in their generations for an everlasting covenant, to be a\\nGod unto thee, and to thy seed after thee.\\n\\n17:8 And I will give unto thee, and to thy seed after thee, the land\\nwherein thou art a stranger, all the land of Canaan, for an\\neverlasting possession; and I will be their God.\\n\\n17:9 And God said unto Abraham, Thou shalt keep my covenant therefore,\\nthou, and thy seed after thee in their generations.\\n\\n17:10 This is my covenant, which ye shall keep, between me and you and\\nthy seed after thee; Every man child among you shall be circumcised.\\n\\n17:11 And ye shall circumcise the flesh of your foreskin; and it shall\\nbe a token of the covenant betwixt me and you.\\n\\n17:12 And he that is eight days old shall be circumcised among you,\\nevery man child in your generations, he that is born in the house, or\\nbought with money of any stranger, which is not of thy seed.\\n\\n17:13 He that is born in thy house, and he that is bought with thy\\nmoney, must needs be circumcised: and my covenant shall be in your\\nflesh for an everlasting covenant.\\n\\n17:14 And the uncircumcised man child whose flesh of his foreskin is\\nnot circumcised, that soul shall be cut '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E19E0M8XzYHc"
      },
      "source": [
        "### Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy6KRpGxWAlS"
      },
      "source": [
        "# First step, initialise tokenizer and get vocabulary\n",
        "# (if you want to train your own tokenizer you can do that here, as long as the\n",
        "# tokenizer has the `id` method to get word ids)\n",
        "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n",
        "vocab = tokenizer.get_vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGf9z4npWA4f"
      },
      "source": [
        "# Second step: create the train and validation datasets as instance of BibleText (just using first 700000 characters for now)\n",
        "bible_train = BibleText(data_raw[:700000], sequence_length=40, tokenizer=tokenizer)\n",
        "bible_val = BibleText(data_raw[700000:750000], sequence_length=40, tokenizer=tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn7EPVKpWAcY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a1266d2-1941-4d1d-9533-60f77bce956d"
      },
      "source": [
        "# check that __len__ method works\n",
        "len(bible_train), len(bible_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(145902, 10236)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXSfUNtElDir",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "85bbc6a3-dcc8-4185-866d-9ea477e418f2"
      },
      "source": [
        "# we can extract an input sequence and check the target word (to check that __getitem__ works)\n",
        "seq_no = 1\n",
        "example_sequence = bible_train[seq_no]\n",
        "_, input_string = ids_to_tokens(example_sequence[0].tolist(), vocab)\n",
        "_, target_word = ids_to_tokens(list([example_sequence[1]]), vocab)\n",
        "print(input_string)\n",
        "print(target_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the king james bible the old testament of the king james bible the first book of moses called genesis in the beginning god created the heaven and the earth and the earth was without form and void and darkness was\n",
            "upon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSkTrAHseGsN"
      },
      "source": [
        "# Third step, initialise the dataloader (this is just an example, we will have a bigger batch later when training the model)\n",
        "bs=5 #batch_size\n",
        "loader_train = DataLoader(bible_train, batch_size=bs, shuffle=False, drop_last=True) # set shuffle to False to sense-check the sequences and targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mDCTHSgdolf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "62764706-ad4b-4cfc-ea8d-5bde928ad018"
      },
      "source": [
        "# and we can check that it works\n",
        "print(next(iter(loader_train)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[  101,  1996,  2332,  2508,  6331,  1996,  2214,  9025,  1997,  1996,\n",
            "          2332,  2508,  6331,  1996,  2034,  2338,  1997,  9952,  2170, 11046,\n",
            "          1999,  1996,  2927,  2643,  2580,  1996,  6014,  1998,  1996,  3011,\n",
            "          1998,  1996,  3011,  2001,  2302,  2433,  1998, 11675,  1998,  4768],\n",
            "        [ 1996,  2332,  2508,  6331,  1996,  2214,  9025,  1997,  1996,  2332,\n",
            "          2508,  6331,  1996,  2034,  2338,  1997,  9952,  2170, 11046,  1999,\n",
            "          1996,  2927,  2643,  2580,  1996,  6014,  1998,  1996,  3011,  1998,\n",
            "          1996,  3011,  2001,  2302,  2433,  1998, 11675,  1998,  4768,  2001],\n",
            "        [ 2332,  2508,  6331,  1996,  2214,  9025,  1997,  1996,  2332,  2508,\n",
            "          6331,  1996,  2034,  2338,  1997,  9952,  2170, 11046,  1999,  1996,\n",
            "          2927,  2643,  2580,  1996,  6014,  1998,  1996,  3011,  1998,  1996,\n",
            "          3011,  2001,  2302,  2433,  1998, 11675,  1998,  4768,  2001,  2588],\n",
            "        [ 2508,  6331,  1996,  2214,  9025,  1997,  1996,  2332,  2508,  6331,\n",
            "          1996,  2034,  2338,  1997,  9952,  2170, 11046,  1999,  1996,  2927,\n",
            "          2643,  2580,  1996,  6014,  1998,  1996,  3011,  1998,  1996,  3011,\n",
            "          2001,  2302,  2433,  1998, 11675,  1998,  4768,  2001,  2588,  1996],\n",
            "        [ 6331,  1996,  2214,  9025,  1997,  1996,  2332,  2508,  6331,  1996,\n",
            "          2034,  2338,  1997,  9952,  2170, 11046,  1999,  1996,  2927,  2643,\n",
            "          2580,  1996,  6014,  1998,  1996,  3011,  1998,  1996,  3011,  2001,\n",
            "          2302,  2433,  1998, 11675,  1998,  4768,  2001,  2588,  1996,  2227]]), tensor([2001, 2588, 1996, 2227, 1997])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAno6ET8fMm2"
      },
      "source": [
        "# The data (if we exclude the targets) has the required shape, that is:\n",
        "# batch_size (5) x sequence_length (10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6X7BOWXmIDf"
      },
      "source": [
        "### Define LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWfs7Qz1bpJ3"
      },
      "source": [
        "class BibleLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_lstm_layers):\n",
        "    super(BibleLSTM, self).__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_lstm_layers = num_lstm_layers\n",
        "\n",
        "    self.embeddings = nn.Embedding(num_embeddings=self.vocab_size,   # This layer requires 2 inputs: the number of possible embeddings\n",
        "                                   embedding_dim=self.embedding_dim) # (i.e. the size of the vocabulary) and the embedding dimension\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim, # The input size needs to be the dimension of the\n",
        "                        num_layers=self.num_lstm_layers, batch_first = True)        # embeddings, while we can set any value for the\n",
        "                                                                                    # hidden layer. Here we're setting the batch to be\n",
        "                                                                                    # the first dimension\n",
        "    \n",
        "    self.fc = nn.Linear(self.hidden_dim, self.vocab_size) # We go from the hidden_dim size to the length of the dictionary to identify\n",
        "                                                           # the most likely word\n",
        "\n",
        "  def forward(self, sequence, previous_state):\n",
        "    out = self.embeddings(sequence)               # The output dimensions will be batch_size x sequence_length x embedding_dim\n",
        "                                                  # As we set batch_first=True we do not need to reshape the data. Otherwise, we\n",
        "                                                  # would need to do out.transpose(0,1)\n",
        "\n",
        "    out, state = self.lstm(out, previous_state)   # The output will have the shape batch_size x sequence_length x hidden_dim.\n",
        "                                                  # The two state tensors (h,c) will have dimensions num_layers x batch_size x hidden_dim\n",
        "\n",
        "    out = out[:, -1, :] # We only want to keep the last LSTM output for each sequence in the batch\n",
        "\n",
        "    out = self.fc(out) # Out will have these dimensions: batch_size x sequence_length x vocab_length - for each word in the each sequence \n",
        "                        # in the batch we will have the corresponding most likely successive word in the dictionary   \n",
        "    return out, state\n",
        "  \n",
        "  def init_state(self, batch_size):\n",
        "    return (torch.zeros(self.num_lstm_layers, batch_size, self.hidden_dim), # These are initialised to have the right dimensions i.e.\n",
        "            torch.zeros(self.num_lstm_layers, batch_size, self.hidden_dim)) # num_layers x batch_size x hidden_dim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep-eUPiYIwTu"
      },
      "source": [
        "### Define optimizer, loss and training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJve7sqPOuMP"
      },
      "source": [
        "# Let's initialise the dataloaders for training and validation sets\n",
        "bs=200 #batch_size\n",
        "loader_train = DataLoader(bible_train, batch_size=bs, shuffle=True, drop_last=True) # set shuffle to False to sense-check the sequences and targets\n",
        "loader_val = DataLoader(bible_val, batch_size=bs, shuffle=True, drop_last=True) # we drop the last batch as it won't be a sequence of the correct length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQkVknv3OG9V"
      },
      "source": [
        "# Initialise the model\n",
        "model = BibleLSTM(vocab_size=len(vocab), embedding_dim=15, hidden_dim=64, num_lstm_layers=1)\n",
        "model = model.to(device=device) # Move model to training device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwcrd3gDOHDk"
      },
      "source": [
        "# Define optimiser and loss\n",
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss = nn.CrossEntropyLoss() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8ZtaS8MOHh-"
      },
      "source": [
        "# Define training loop \n",
        "\n",
        "def train(n_epochs, optimizer, model, loss_fn, dataloader_train, dataloader_val):\n",
        "  \n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    model.train() # Set the model to training mode - this has an impact on the behaviour of some modules\n",
        "    loss_train = 0 # initialise the loss\n",
        "    \n",
        "    # calculate number of training and validation batches for loss normalisation (for comparison purposes)\n",
        "    n_training_batches = int(len(loader_train.dataset) / loader_train.batch_size)\n",
        "    n_validation_batches = int(len(loader_val.dataset) / loader_val.batch_size) \n",
        "\n",
        "    state_h, state_c = model.init_state(dataloader_train.batch_size) # initialise the hidden state\n",
        "    state_h = state_h.to(device=device) # move states to training device\n",
        "    state_c = state_c.to(device=device) # move states to training device\n",
        "\n",
        "    for sequences, targets in dataloader_train:\n",
        "\n",
        "      sequences = sequences.to(device=device) # move sequences to training device\n",
        "      targets = targets.to(device=device) # move targets to training device\n",
        "\n",
        "      outputs, (state_h, state_c) = model(sequences, (state_h, state_c)) # pass the batch through the model \n",
        "\n",
        "      training_loss = loss_fn(outputs, targets) # compute the loss\n",
        "\n",
        "      state_h = state_h.detach() # excluding tensor from gradient calculations\n",
        "      state_c = state_c.detach() # excluding tensor from gradient calculations\n",
        "\n",
        "      optimizer.zero_grad() # reset the gradient from the last round\n",
        "      training_loss.backward() # backprop\n",
        "      optimizer.step() # update weights based on gradient\n",
        "  \n",
        "      loss_train += training_loss.item() # transforming loss to python number to escape the gradients\n",
        "\n",
        "    # calculate validation loss for this epoch\n",
        "    with torch.no_grad():\n",
        "      model.eval() \n",
        "      loss_val = 0\n",
        "      for val_sequences, val_targets in dataloader_val:\n",
        "          val_sequences = sequences.to(device=device) # move validation sequences to training device\n",
        "          val_targets = targets.to(device=device) # move validation targets to training device\n",
        "          val_outputs, _ = model(val_sequences, (state_h, state_c)) # use the last h and c states from this epoch's training \n",
        "          validation_loss = loss_fn(val_outputs, val_targets)\n",
        "          loss_val += validation_loss.item()\n",
        "    \n",
        "    # monitor training \n",
        "    loss_train = loss_train / n_training_batches\n",
        "    loss_val = loss_val / n_validation_batches\n",
        "    print(f'Epoch: {epoch}, Tranining loss: {loss_train}, Validation loss: {loss_val}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIePiSDuXiUq"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0m7ibokOHAw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "fc281fa1-424c-4347-dfc5-fd4c77992d1f"
      },
      "source": [
        "# Run training\n",
        "train(50, optimizer, model, loss, loader_train, loader_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Tranining loss: 6.152591883236815, Validation loss: 5.401205062866211\n",
            "Epoch: 2, Tranining loss: 5.380418609362734, Validation loss: 4.9138875007629395\n",
            "Epoch: 3, Tranining loss: 5.022629722155661, Validation loss: 4.674881935119629\n",
            "Epoch: 4, Tranining loss: 4.77682234917158, Validation loss: 4.663574695587158\n",
            "Epoch: 5, Tranining loss: 4.584698462518972, Validation loss: 4.278537750244141\n",
            "Epoch: 6, Tranining loss: 4.429215103183427, Validation loss: 4.245894432067871\n",
            "Epoch: 7, Tranining loss: 4.297662465824184, Validation loss: 4.341514587402344\n",
            "Epoch: 8, Tranining loss: 4.184844612257634, Validation loss: 3.9926724433898926\n",
            "Epoch: 9, Tranining loss: 4.085019350705977, Validation loss: 3.9995639324188232\n",
            "Epoch: 10, Tranining loss: 3.9952887501082137, Validation loss: 3.7836692333221436\n",
            "Epoch: 11, Tranining loss: 3.91458703559122, Validation loss: 4.052802085876465\n",
            "Epoch: 12, Tranining loss: 3.8409697168975536, Validation loss: 3.5727720260620117\n",
            "Epoch: 13, Tranining loss: 3.7738939338914324, Validation loss: 3.6379380226135254\n",
            "Epoch: 14, Tranining loss: 3.71325699193978, Validation loss: 3.670772075653076\n",
            "Epoch: 15, Tranining loss: 3.657626884627898, Validation loss: 3.628553867340088\n",
            "Epoch: 16, Tranining loss: 3.6063464943764143, Validation loss: 3.3982274532318115\n",
            "Epoch: 17, Tranining loss: 3.5596596847999895, Validation loss: 3.627471685409546\n",
            "Epoch: 18, Tranining loss: 3.5170754601435408, Validation loss: 3.4660232067108154\n",
            "Epoch: 19, Tranining loss: 3.4772112690045183, Validation loss: 3.433833360671997\n",
            "Epoch: 20, Tranining loss: 3.4394188607507608, Validation loss: 3.4590964317321777\n",
            "Epoch: 21, Tranining loss: 3.405025259784547, Validation loss: 3.3914077281951904\n",
            "Epoch: 22, Tranining loss: 3.37216531330993, Validation loss: 3.7299201488494873\n",
            "Epoch: 23, Tranining loss: 3.3414798129569685, Validation loss: 3.5679454803466797\n",
            "Epoch: 24, Tranining loss: 3.3120582761097346, Validation loss: 3.517155408859253\n",
            "Epoch: 25, Tranining loss: 3.2847022519680698, Validation loss: 3.3862242698669434\n",
            "Epoch: 26, Tranining loss: 3.2591871482681674, Validation loss: 3.3309874534606934\n",
            "Epoch: 27, Tranining loss: 3.234371308273085, Validation loss: 3.491192579269409\n",
            "Epoch: 28, Tranining loss: 3.2107105552741366, Validation loss: 3.346479892730713\n",
            "Epoch: 29, Tranining loss: 3.1886189844055592, Validation loss: 3.1075384616851807\n",
            "Epoch: 30, Tranining loss: 3.166955330734881, Validation loss: 3.183797597885132\n",
            "Epoch: 31, Tranining loss: 3.146745011966742, Validation loss: 3.0389275550842285\n",
            "Epoch: 32, Tranining loss: 3.1273100840538453, Validation loss: 3.1353957653045654\n",
            "Epoch: 33, Tranining loss: 3.1083743035875067, Validation loss: 2.93183970451355\n",
            "Epoch: 34, Tranining loss: 3.0901722155807767, Validation loss: 2.9019806385040283\n",
            "Epoch: 35, Tranining loss: 3.0731493452121854, Validation loss: 3.2521915435791016\n",
            "Epoch: 36, Tranining loss: 3.0567375465675637, Validation loss: 3.1851894855499268\n",
            "Epoch: 37, Tranining loss: 3.0400076955761275, Validation loss: 2.855480432510376\n",
            "Epoch: 38, Tranining loss: 3.025029044746535, Validation loss: 3.1768813133239746\n",
            "Epoch: 39, Tranining loss: 3.0094691532956555, Validation loss: 2.916607141494751\n",
            "Epoch: 40, Tranining loss: 2.994769021480335, Validation loss: 2.7250683307647705\n",
            "Epoch: 41, Tranining loss: 2.9810137964570473, Validation loss: 3.1600534915924072\n",
            "Epoch: 42, Tranining loss: 2.967744338332544, Validation loss: 2.8496453762054443\n",
            "Epoch: 43, Tranining loss: 2.9544878264350003, Validation loss: 2.891867160797119\n",
            "Epoch: 44, Tranining loss: 2.941930572355399, Validation loss: 2.8476943969726562\n",
            "Epoch: 45, Tranining loss: 2.928991317421975, Validation loss: 2.6224310398101807\n",
            "Epoch: 46, Tranining loss: 2.9169045053882363, Validation loss: 3.005356788635254\n",
            "Epoch: 47, Tranining loss: 2.904998448323484, Validation loss: 3.0358307361602783\n",
            "Epoch: 48, Tranining loss: 2.893433338003067, Validation loss: 2.956752300262451\n",
            "Epoch: 49, Tranining loss: 2.8821692087359225, Validation loss: 2.915318250656128\n",
            "Epoch: 50, Tranining loss: 2.8709100309876914, Validation loss: 2.786431312561035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqqPVPfCAeF9"
      },
      "source": [
        "### Save (and load) the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2OvAEM9ryx1"
      },
      "source": [
        "# We can save the model weights locally as follows:\n",
        "torch.save(model.state_dict(), 'trained_lstm.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egrXWao7uisN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4649fabe-216f-4661-bf94-43e0264f8f9c"
      },
      "source": [
        "# To load the model we first need to initialise it. Note that the class needs to \n",
        "# have been defined in exactly the same way as that whose weights have been saved\n",
        "loaded_model = BibleLSTM(vocab_size=len(vocab), embedding_dim=15, hidden_dim=64, num_lstm_layers=1)\n",
        "loaded_model = loaded_model.to(device=device)\n",
        "loaded_model.load_state_dict(torch.load('trained_lstm.pt', map_location=device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqECmX1KJFvB"
      },
      "source": [
        "### Use trained model for predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49Uiuv-kJDbc"
      },
      "source": [
        "# This function predicts the next n word given a trained model and an input sequence\n",
        "# (provided as output of a dataloader)\n",
        "def predict(trained_model, input_sentence, n_words=1, temperature=0):\n",
        "  \"\"\"\n",
        "  Given a trained model and input sentence and number of words predicts the next n words\n",
        "  Relies on BibleText dataset class, as well as tokenizer, its vocab and id_to_tokens function\n",
        "  The temperature parameter introduces an element of randomness in the prediction\n",
        "  \"\"\"\n",
        "  # First convert the input sentence into a tensor of the right dimensions to be fed into the model\n",
        "  # (dimensions are n_batches (in this case 1) x sequence_length)\n",
        "  dataset = BibleText(input_sentence, len(input_sentence.split()), tokenizer)\n",
        "  loader = DataLoader(dataset, 1) # just one batch\n",
        "  for first_part, second_part in loader:\n",
        "    sequence = torch.cat((first_part.view(-1), torch.as_tensor(second_part)), 0).view(1,-1)\n",
        "\n",
        "  # We also initialise a dummy hidden state for the model\n",
        "  with torch.no_grad():\n",
        "    # initialise dummy hidden state for model\n",
        "    state_h, state_c = trained_model.init_state(1) # one batch as we just have a sentence\n",
        "\n",
        "  # predict and add to predicted sentence as many times as needed\n",
        "  predicted_tokens = []\n",
        "  for word in range(1, n_words+1):\n",
        "           \n",
        "    # make prediction\n",
        "    out, _ = trained_model(sequence, (state_h, state_c)) # get distribution over words in dictionary\n",
        "    randomness = torch.rand(out.shape) * temperature\n",
        "    out_rand = out + randomness\n",
        "    predicted_token = out_rand.argmax().item() # get the most likely one as integer (ndex)\n",
        "    predicted_tokens.append(predicted_token)\n",
        "\n",
        "    # Add to the sequence\n",
        "    sequence = torch.cat((sequence.view(-1), torch.as_tensor([predicted_token])), 0).view(1,-1)\n",
        "\n",
        "  # output final sequence\n",
        "  _, predicted_sequence = ids_to_tokens(predicted_tokens, vocab) # find corresponding words in dictionary\n",
        "\n",
        "  return predicted_sequence\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSxphdjcivtn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0e704593-8ac5-4ae1-bd30-d3b347fd521c"
      },
      "source": [
        "# And finally we can make a prediction for the number of words that we want\n",
        "model_for_prediction = model.to('cpu')\n",
        "seed = \"\"\"And Abram was fourscore and six years old, when Hagar bare\\nIshmael to Abram.\\n\\n17:1 \n",
        "          And when Abram was ninety years old and nine, the LORD appeared\\nto Abram, and said unto him,\n",
        "       \"\"\"\n",
        "predict(model, seed, n_words=100, temperature=3.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'all my hand and they shall come to pass for him thou sha ##lt not uncover her naked ##ness and i am the lord and he called the name of the people and the lord said unto bala ##k and the lord said unto moses and he said unto her father shall be the first ##born of your own country and we have heard the land of egypt 4 and the egyptians said unto him take us to pass when i have brought you out of the children of israel and say unto him and the lord said unto the'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3O7J5_-82du",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e447ace3-8e50-4e26-e52c-b14f32d4b175"
      },
      "source": [
        "seed = \"\"\"In the beginning there was not much to go around, quite a lot of darkness and boredom\"\"\"\n",
        "predict(model, seed, n_words=20, temperature=3.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the sons of levi and upward all the children of israel and moses took to the levi ##tes according to'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcKoFF7t_tlh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}