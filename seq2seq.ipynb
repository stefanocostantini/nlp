{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IvMmcu4goSWnQV1TAa8Zdyvn8ykDo-0e",
      "authorship_tag": "ABX9TyNciJjOgFapJE6MITSnTYgU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanocostantini/nlp/blob/main/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF1pVlL-vadk"
      },
      "source": [
        "## Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmTOpeXDve6P"
      },
      "source": [
        "- This notebook contains an implementation of the Seq2Seq architecture. \n",
        "- We will use it to set up a NMT model, translating from Italian to English.\n",
        "- The dataset used for model training is a collection of transcripts of the European Parliament sessions since 2000. The data (for multiple data pairs) is available here: http://www.statmt.org/europarl/\n",
        "- For this model, we will use a shorter version of the Italian-English dataset containing 50,000 examples. This has been pre-processed and uploaded to Drive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md1QOXscZypf"
      },
      "source": [
        "### Installs & imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wEj9vi0Z3FL"
      },
      "source": [
        "import spacy\n",
        "import random\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import TranslationDataset\n",
        "from torch import nn\n",
        "from torch import optim # for the optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frKB80lFb7K0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2296aa6-f28a-4c4f-de95-be981a34a9be"
      },
      "source": [
        "# Download spacy components for languages of interest\n",
        "!python -m spacy download it\n",
        "!python -m spacy download en\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting it_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.2.5/it_core_news_sm-2.2.5.tar.gz (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 24.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from it_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->it_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->it_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->it_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: it-core-news-sm\n",
            "  Building wheel for it-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for it-core-news-sm: filename=it_core_news_sm-2.2.5-cp36-none-any.whl size=14471131 sha256=51104bae178c5e8d24a791d1490b94da01488e6b770a80aa338a76a5ebb14141\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h4ida3a_/wheels/a1/01/c2/127ab92cc5e3c7f36b5cd4bff28d1c29c313962a2ba913e720\n",
            "Successfully built it-core-news-sm\n",
            "Installing collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/it_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/it\n",
            "You can now load the model via spacy.load('it')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWgXFIFpaBfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a63b36-6e30-4cbd-8b44-797c12bcdf17"
      },
      "source": [
        "# Initialise seed\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f52b44aaee8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTYBGkRQhr7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3646b80-232b-48cf-840d-47e6e990a3ff"
      },
      "source": [
        "# We also make sure that we can use the GPU if it is available:\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig9gAwHX7OJW"
      },
      "source": [
        "### Get data onto colab's filesystem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhDr5S_Z_iy6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919c3a63-1283-4b9d-f6f7-6dcc51e718b2"
      },
      "source": [
        "# First mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydy6bP0H9y_p"
      },
      "source": [
        "# Then we create the folder for our data in the local filesystem\n",
        "!mkdir /content/translation/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmFIM3rsvWwO"
      },
      "source": [
        "# And finally we copy the files across\n",
        "# Short version\n",
        "!cp /content/drive/My\\ Drive/ML/translation/short/europarl-v7.it-en-SHORT-ENGLISH.txt /content/translation\n",
        "!cp /content/drive/My\\ Drive/ML/translation/short/europarl-v7.it-en-SHORT-ITALIAN.txt /content/translation\n",
        "\n",
        "# Very short version\n",
        "!cp /content/drive/My\\ Drive/ML/translation/very_short/europarl-v7.it-en-VERY-SHORT-ENGLISH.txt /content/translation\n",
        "!cp /content/drive/My\\ Drive/ML/translation/very_short/europarl-v7.it-en-VERY-SHORT-ITALIAN.txt /content/translation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi8Xf1dS-ySD"
      },
      "source": [
        "# Files will be available at these paths\n",
        "# Short version\n",
        "english_raw_path = \"/content/translation/europarl-v7.it-en-SHORT-ENGLISH.txt\"\n",
        "italian_raw_path = \"/content/translation/europarl-v7.it-en-SHORT-ITALIAN.txt\"\n",
        "\n",
        "# Very short version\n",
        "# english_raw_path = \"/content/translation/europarl-v7.it-en-VERY-SHORT-ENGLISH.txt\"\n",
        "# italian_raw_path = \"/content/translation/europarl-v7.it-en-VERY-SHORT-ITALIAN.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ssy6myGK-Mv"
      },
      "source": [
        "### Setting up the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QY5vegQM0By"
      },
      "source": [
        "Now we can import the data and set up the dataset. For this, we will use the package `torchtext` which makes things much easier.\n",
        "\n",
        "The steps involved are as follows:\n",
        "1. set up the tokenizer functions for the specific languages we're interested in - we will build this on top of Spacy\n",
        "2. define the data `Fields` (from the `torchtext` library) which will detail the operations we want to do on the text datasets\n",
        "3. set up a the training, validation and test dataset which will be needed to train the model and evaluate it. We will also create the vocabularies for each language.\n",
        "4. Define a `DataLoader` to generate data batches. In this case we will use `BucketIterator` from `torchtext`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUxJ-H8rktln"
      },
      "source": [
        "#### 1. Set up the tokenizer functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw8kD8OakGZ6"
      },
      "source": [
        "# Let's load the spacy components and set up the tokenizer functions\n",
        "spacy_it = spacy.load('it')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "# NOTE: we're going to translate from IT to EN. A way to improve the model performance is to\n",
        "# reverse the sequence of the source sentence so that the final RNN hidden state of the encoder\n",
        "# is more directly affected by the first word of the source sentence, which is likely to have an \n",
        "# stronger relationship to the first word(s) of the target sentence \n",
        "def tokenize_it(text): \n",
        "  return [token.text for token in spacy_it.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]\n",
        "\n",
        "# (note: torchtext has a tokenizer, but at the moment it would appear it doesn't support Italian - though need to doublecheck)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXtgkmLslmUa"
      },
      "source": [
        "#### 2. Set up the data `Fields`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPZijgf96iWH"
      },
      "source": [
        "We note set up the text data `Fields` (https://pytorch.org/text/data.html#field), which is a way to define a datatype as well as operations that need to be apply to it (e.g. tokenisation, adding tokens, lower case, etc.). We want to do the same processing to both source and target text, but as the tokenizer function is language specific, we need to define two separate `Fields`, which we call `source` (for Italian) and `target` (for English)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YAF_6xLlfjS"
      },
      "source": [
        "source = Field(tokenize=tokenize_it, init_token='<sos>', eos_token='<eos>', lower=True)\n",
        "target = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYrMDRvg-NsW"
      },
      "source": [
        "This is all we need for now, we can now move to set up the `Dataset`, loading in the data imported above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFMvwkXs_AQd"
      },
      "source": [
        "#### 3. Set up dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq5mJ6Wb_DuO"
      },
      "source": [
        "`torchtext` has the `TranslationDataset` class (https://pytorch.org/text/datasets.html#torchtext.datasets.TranslationDataset) which can be used to build a dataset specifically for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok5uhbTo9hAh"
      },
      "source": [
        "# Short version\n",
        "data = TranslationDataset(\"/content/translation/\",\n",
        "                         (\"europarl-v7.it-en-SHORT-ITALIAN.txt\", \"europarl-v7.it-en-SHORT-ENGLISH.txt\"),\n",
        "                         (source, target))\n",
        "\n",
        "# Very short version\n",
        "# data = TranslationDataset(\"/content/translation/\",\n",
        "#                          (\"europarl-v7.it-en-VERY-SHORT-ITALIAN.txt\", \"europarl-v7.it-en-VERY-SHORT-ENGLISH.txt\"),\n",
        "#                          (source, target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8xPMhV3Plh1"
      },
      "source": [
        "We can now split the dataset into training, validation and test. To do so we use `torchtext.data.Dataset` split method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVOrx1rlcPNo"
      },
      "source": [
        "# We can now use the `split` method to generate training, validation and test data \n",
        "train_data, val_data, test_data = data.split(split_ratio=[0.7, 0.15, 0.15], strata_field='source')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTKmav7vIaM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6d9af7-1700-42e3-c846-f72d0a87b324"
      },
      "source": [
        "# We can extract individual examples from the dataset, for either language using the\n",
        "# .srt or .trg methods. This shows that the datasets are aligned correctly\n",
        "print(train_data[222].src[::-1])\n",
        "print(train_data[222].trg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'nostri', 'stessi', 'dirigenti', 'sono', 'degli', 'irresponsabili', '.']\n",
            "['our', 'own', 'political', 'leaders', 'are', 'acting', 'irresponsibly', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiYdKd0qMD14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb08048c-18cb-4d62-ecac-60fd9d80a94c"
      },
      "source": [
        "# These are the dimensions of the datasets\n",
        "print(f'Size of train dataset: {len(train_data)}')\n",
        "print(f'Size of validation dataset: {len(val_data)}')\n",
        "print(f'Size of test dataset: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of train dataset: 34825\n",
            "Size of validation dataset: 7463\n",
            "Size of test dataset: 7462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPTZDK4rXXfA"
      },
      "source": [
        "Now that we have the data, the last thing to do is build a vocabulary for both the source and the target datasets. We build it using the training dataset to avoid any data leakage. We also set a minimum frequence of 2, not include those words that appear only once. These will be treated as `<unk>` going forward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAqhIyEgTADT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a959cb4c-425d-4263-c9e3-34d7564b84e3"
      },
      "source": [
        "source.build_vocab(train_data, min_freq=2)\n",
        "target.build_vocab(train_data, min_freq=2)\n",
        "print(f'The source vocabulary contains {len(source.vocab)} unique words')\n",
        "print(f'The target vocabulary contains {len(target.vocab)} unique words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The source vocabulary contains 17436 unique words\n",
            "The target vocabulary contains 11986 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QzjIwgEfcPe"
      },
      "source": [
        "#### 4. Set up the data loader (`BucketIterator` in this case)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icNkxLPyhfep"
      },
      "source": [
        "For text we use the `BucketIterator` which does the following:\n",
        "- generates batches with the `.src` and `.trg` properties\n",
        "- numericalises sequences (i.e. replaces tokens with indices - using the specific vocabulary for each of the `Fields`)\n",
        "- automatically pads sentences in a batch so that they are all of the same length of the longest sequence\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYzRwGQ-X9MN"
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "\n",
        "train_iter = BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\n",
        "val_iter = BucketIterator(val_data, batch_size=BATCH_SIZE, device=device)\n",
        "test_iter = BucketIterator(test_data, batch_size=BATCH_SIZE, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc2wDMZPYqyz"
      },
      "source": [
        "# We can see that the length of the sentences in each batch is automatically set to\n",
        "# that of the longest sentence\n",
        "batch = next(iter(train_iter))\n",
        "it_example = batch.src\n",
        "en_example = batch.trg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfHvlxDH1-fL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8978060e-4db2-417f-a061-ace77a59564c"
      },
      "source": [
        "# The iterator makes sure that all sequences in the batch are as long as the longest sentence in it.\n",
        "print(it_example.shape, en_example.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([95, 100]) torch.Size([102, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugz-AmjHghfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d331108-0813-4a9b-bb2d-c1c88439cbe0"
      },
      "source": [
        "# We can check what word the various tokens correspond to (and viceversa which token words correspond to)\n",
        "# For example, all sentences start with token '2', which as expected is '<sos>'\n",
        "print(source.vocab.itos[2])\n",
        "# Then sentences are finished with token '3', which is '<eos>'\n",
        "print(source.vocab.itos[3])\n",
        "# After that point, the sentence is padded with token '1', which is '<pad>' to make it of the same length as the longest sentence\n",
        "print(source.vocab.itos[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos>\n",
            "<eos>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4om-7iDmjQP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0320778d-9808-437c-98bd-563bf7742d13"
      },
      "source": [
        "# We can use the lookup to see which word corresponds to which index:\n",
        "print(source.vocab.itos[1234])\n",
        "print(target.vocab.itos[5678])\n",
        "# We can also lookup the number corresponding to each word\n",
        "print(source.vocab.stoi[\"parlamento\"])\n",
        "print(target.vocab.stoi[\"parliament\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "statuto\n",
            "biotechnology\n",
            "43\n",
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RejP0kyW5WWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c278e28-c3e2-4407-cf7a-22a47fc2bef4"
      },
      "source": [
        "# We can also reconstruct the words of a specific sequence. Let's take the first sentence\n",
        "# of the batch for both source (italian) and target (english). Note that the source's order is inverted as required.\n",
        "# Italian\n",
        "print([source.vocab.itos[it_example[i,1].item()] for i in range(0, it_example.shape[0])])\n",
        "# English\n",
        "print([target.vocab.itos[en_example[i,1].item()] for i in range(0, en_example.shape[0])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<sos>', '.', 'unito', 'regno', 'del', 'favore', 'a', 'comunitario', 'bilancio', 'al', 'finanziamento', 'del', 'correzione', 'di', 'meccanismo', 'il', 'contro', ',', 'casi', 'alcuni', 'in', ',', 'espresso', 'appena', 'hanno', 'che', 'voto', 'il', 'per', 'britannici', 'socialisti', 'colleghi', 'miei', 'dei', 'imbarazzo', \"'\", 'l', 'sopportare', 'potrei', 'non', 'ma', ',', 'così', 'è', 'non', 'che', 'certo', 'sono', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<sos>', 'i', 'am', 'sure', 'you', 'are', 'not', ',', 'but', 'i', 'would', 'hate', 'my', 'british', 'socialist', 'colleagues', 'to', 'be', 'embarrassed', 'by', 'the', 'way', 'they', 'have', 'just', 'voted', ',', 'in', 'some', 'cases', 'against', 'the', 'uk', 'budget', 'rebate', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPYJN1PxpmFR"
      },
      "source": [
        "### 5. Setting up the Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K62Zp1EL_fvO"
      },
      "source": [
        "We can now set up the Seq2Seq model. This is made up by three components:\n",
        "\n",
        "1. the Encoder (an LSTM model)\n",
        "2. the Decoder (an LSTM model, which initialises its hidden state using the last hidden state of the Encoder, i.e. in the case of an encoder LSTM it would be its cell and its hidden states. Note also that it could be another type of model like a GRU)\n",
        "3. the Seq2Seq model itself that bring the two models together "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAQRfahExjpv"
      },
      "source": [
        "#### Encoder\n",
        "\n",
        "The encoder is a standard RNN, in this case an LSTM. For simplicity we just use one LSTM layer. We're not interested in the output of the LSTM in this case, just its hidden and cell states. So we discard the model's output at each iteration (and we don't need a final fully connected layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76pLM0mCT1Hn"
      },
      "source": [
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_lstm_layers):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size           # sometimes also labelled 'input size' as this is the dimentionality of the input tokens\n",
        "    self.embedding_dim = embedding_dim     # the size of the embeddings\n",
        "    self.hidden_dim = hidden_dim           # the size of the hidden layer\n",
        "    self.num_lstm_layers = num_lstm_layers # the number of layers used\n",
        "\n",
        "    self.embeddings = nn.Embedding(num_embeddings=self.vocab_size,   # This layer requires 2 inputs: the number of possible embeddings\n",
        "                                   embedding_dim=self.embedding_dim) # (i.e. the size of the vocabulary) and the embedding dimension\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim, # The input size needs to be the dimension of the\n",
        "                        num_layers=self.num_lstm_layers)                            # embeddings, while we can set any value for the\n",
        "                                                                                    # hidden layer. \n",
        "\n",
        "  def forward(self, source_sequence):\n",
        "    embedded = self.embeddings(source_sequence)        # The output dimensions will be  sequence_length x batch_size x embedding_dim                           \n",
        "\n",
        "    _, (hidden, cell) = self.lstm(embedded)          # We do not care about the output in this case, just the hidden and cell states\n",
        "                                                     # These will have dimensions num_layers x batch_size x hidden_dim\n",
        "    return hidden, cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FfWwIAn_fLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "884807cf-cc24-4565-b466-5d834cbacf7e"
      },
      "source": [
        "# We can now test it and print out the dimensions of the hidden and cell states.\n",
        "# These are [num_layers x batch_size x hidden_dim]\n",
        "encoder = Encoder(vocab_size=len(source.vocab), embedding_dim=128, hidden_dim=256, num_lstm_layers=1).to(device)\n",
        "hidden_enc, cell_enc = encoder(it_example)\n",
        "print(hidden_enc.shape, cell_enc.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100, 256]) torch.Size([1, 100, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1KOzKbHpro-"
      },
      "source": [
        "#### Decoder\n",
        "\n",
        "In this case, the first inputs to the decoder will be:\n",
        "\n",
        "- The first token of the target sequence (as the model, given this token, will learn to predict the next one - in our case it will be a `<sos>` token)\n",
        "- The final cell and hidden states of the encoder module\n",
        "\n",
        "The model will need to produce an output (prediction, so we will need to introduce a fully connected layer mapping from the hidden state to vocabulary i.e. `hidden_dim --> vocab_size`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okV1-byJxmpt"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, num_lstm_layers):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size           # sometimes also labelled 'input size' as this is the dimentionality of the input tokens\n",
        "    self.embedding_dim = embedding_dim     # the size of the embeddings\n",
        "    self.hidden_dim = hidden_dim           # the size of the hidden layer\n",
        "    self.num_lstm_layers = num_lstm_layers # the number of layers used\n",
        "    \n",
        "    \n",
        "    self.embeddings = nn.Embedding(num_embeddings=self.vocab_size,   # This layer requires 2 inputs: the number of possible embeddings\n",
        "                                   embedding_dim=self.embedding_dim) # (i.e. the size of the vocabulary) and the embedding dimension\n",
        "    \n",
        "    self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim, # The input size needs to be the dimension of the\n",
        "                        num_layers=self.num_lstm_layers)                            # embeddings, while we can set any value for the\n",
        "                                                                                    # hidden layer. \n",
        "    \n",
        "    self.fc = nn.Linear(in_features=self.hidden_dim, out_features=self.vocab_size)  # In this case at each iteration we make a prediction\n",
        "                                                                                    # so we need to map the hidden state onto the vocab size\n",
        "\n",
        "  def forward(self, hidden, cell, target_sequence):\n",
        "    embedded = self.embeddings(target_sequence.unsqueeze(0))      # Need to add dimension of size one. Embeddings will have size embedding_dim                                      # max_length x batch_size x embedding size\n",
        "    out, (hidden, cell) = self.lstm(embedded, (hidden, cell))     # Hidden states will have dimensions num_layers x batch_size x hidden_dim\n",
        "                                                                  # Output tensor will have dimensions max_length X batch_size x hidden_dim\n",
        "    \n",
        "    prediction = self.fc(out.squeeze(0))                          # Predictions will have dimensions batch_size x vocab_size\n",
        "\n",
        "    return prediction, hidden, cell                                                                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaQlJ1Mvxmv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180ad659-1684-4eea-f142-c56117ad6947"
      },
      "source": [
        "# We can now test it and print out the dimensions of the output, hidden and cell states.\n",
        "# These will be\n",
        "decoder = Decoder(vocab_size=len(target.vocab), embedding_dim=128, hidden_dim=256, num_lstm_layers=1).to(device)\n",
        "prediction_dec, hidden_dec, cell_dec = decoder(hidden_enc, cell_enc, en_example[0]) # passing only one token from target sentence\n",
        "print(prediction_dec.shape, hidden_enc.shape, cell_enc.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100, 11986]) torch.Size([1, 100, 256]) torch.Size([1, 100, 256])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTL8Pbtzpp4E"
      },
      "source": [
        "#### Full model\n",
        "\n",
        "We can now join the two components together in the Seq2Seq model. \n",
        "- There is no need to define any further layers as the `Encoder` and `Decoder` classes already provide everything we need. \n",
        "- The only check to make is to make sure that both the hidden dimension and the number of layers of `Encoder` and `Decoder` are the same.\n",
        "\n",
        "After that, the model will work as follows:\n",
        "\n",
        "- The forward method takes the source and target batches of sequences as inputs\n",
        "- First, the source batch is passed through the `Encoder` to obtain the hidden state which will be used to initialise the decoder\n",
        "- Then, the target sequence is passed, **token by token** through the decoder. Note that this is the standard approach. However, a technique to make the model converge faster is **teacher forcing** where instead of inputing the next token in the sequence, the model is fed its prediction from the previous step. In this case, we add a parameters that will use teacher forcing with on a fixed % of iterations.\n",
        "\n",
        "Here's an example\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*KtWwvLK-jpGPSnj3tStg-Q.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjfg4UOE3UWd"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder=Encoder, decoder=Decoder, device=device):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    assert(encoder.hidden_dim==decoder.hidden_dim)\n",
        "    assert(encoder.num_lstm_layers==decoder.num_lstm_layers)\n",
        "\n",
        "  def forward(self, source_sequence, target_sequence, teacher_forcing_ratio=0.5):\n",
        "    # Note that this is done in batches, so one iteration of the code below\n",
        "    # will go through as many sequences as there are in batch\n",
        "\n",
        "    # Initialise empty tensor for predictions\n",
        "    # dimensions will be max_length x batch_size x vocab_size\n",
        "    max_length, batch_size = target_sequence.shape \n",
        "    vocab_size = self.decoder.vocab_size # need to use the vocab size of the target corpus as the\n",
        "                                         # predictions will be made from this dictionary\n",
        "   \n",
        "    predictions = torch.zeros(max_length, batch_size, vocab_size).to(device)\n",
        "\n",
        "    # Get source sequence through encoder\n",
        "    hidden, cell = self.encoder(source_sequence)\n",
        "\n",
        "    # Get target sequence through decoder, token by token\n",
        "    trg = target_sequence[0] # start from the first token\n",
        "    for i in range(1, max_length): # then loop through all the other tokens\n",
        "      output, hidden, cell = self.decoder(hidden, cell, trg)\n",
        "      predictions[i] = output \n",
        "\n",
        "      if random.random() < teacher_forcing_ratio:\n",
        "        trg = target_sequence[i] # in this case we feed in the next actual token in the target sequence\n",
        "      else:\n",
        "        trg = output.argmax(1) # in this case we feed in the predicted token\n",
        "      \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQtW0pwI3UdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c51b5379-fcce-46fa-904b-24c12f86e78c"
      },
      "source": [
        "# Let's now check the output of the whole network\n",
        "encoder = Encoder(vocab_size=len(source.vocab), embedding_dim=128, hidden_dim=256, num_lstm_layers=1).to(device)\n",
        "decoder = Decoder(vocab_size=len(target.vocab), embedding_dim=128, hidden_dim=256, num_lstm_layers=1).to(device)\n",
        "seq2seq = Seq2Seq(encoder, decoder, device).to(device)\n",
        "seq2seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embeddings): Embedding(17436, 128)\n",
              "    (lstm): LSTM(128, 256)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embeddings): Embedding(11986, 128)\n",
              "    (lstm): LSTM(128, 256)\n",
              "    (fc): Linear(in_features=256, out_features=11986, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl1U_gA53Uba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc78ae3-1568-4685-a465-512e7241752f"
      },
      "source": [
        "predictions = seq2seq(it_example, en_example)\n",
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([102, 100, 11986])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSQItbODmHYp"
      },
      "source": [
        "As expected, the predictions have dimensions `max_length` x `batch_size` x `target.vocab_size`. Now, by picking, for each token, the probability with the highest weight, we can extract a seriens of tokens that can be compared with the ground truth (the output sequence batch)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAc8Oxjc3UPL"
      },
      "source": [
        "# For simplicity, let's just focus on the first sequence of the predicted batch and compare it\n",
        "# with the first sequence of the target batch\n",
        "first_source = it_example[:,1]\n",
        "first_predicted = predictions[:,1].argmax(1)\n",
        "first_target = en_example[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2otMTRdwqQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b953bf2-0119-4c05-e809-482c4e7473ff"
      },
      "source": [
        "# And we can print out the actual sentences as follows\n",
        "print((\" \").join([source.vocab.itos[first_source[i].item()] for i in range(0, first_source.shape[0])][::-1]))\n",
        "print((\" \").join([target.vocab.itos[first_target[i].item()] for i in range(0, first_target.shape[0])]))\n",
        "print((\" \").join([target.vocab.itos[first_predicted[i].item()] for i in range(0, first_predicted.shape[0])]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <eos> sono certo che non è così , ma non potrei sopportare l ' imbarazzo dei miei colleghi socialisti britannici per il voto che hanno appena espresso , in alcuni casi , contro il meccanismo di correzione del finanziamento al bilancio comunitario a favore del regno unito . <sos>\n",
            "<sos> i am sure you are not , but i would hate my british socialist colleagues to be embarrassed by the way they have just voted , in some cases against the uk budget rebate . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "<unk> went historical geographical diversified prepare causes dishonest requests pedro barents commons perform proclaiming tumble illiterates expresses mask empowered helmut elisabeth initiatives arrogance outwith contravention cabinet publishing occurred liberation enthusiastically cycle horse arrogance kilometre sympathetic baringdorf perform turn assert speeding speeding speeding worded worded worded cyprus speeding promote speeding promote com talks alliance alliance worded worded cyprus speeding speeding promote speeding promote speeding speeding worded worded cyprus speeding promote speeding promote com fingerprinting structure smacks sab4 sab4 sab4 sab4 speeding promote com fingerprinting structure smacks awaits sab4 iii jeopardy speeding amalgamation speeding speeding promote com talks alliance alliance worded worded worded worded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV-Jhgb8yauE"
      },
      "source": [
        "Of course the \"predicted\" text makes no sense as the model has not been trained yet. And note also that the model has not yet learnt when to stop the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08LNygmspzTj"
      },
      "source": [
        "### 6. Model training\r\n",
        "\r\n",
        "At this stage, we just need to train the model (and evaluate it on the validation set). What we need is:\r\n",
        "\r\n",
        "0. an initialised model (we already have it from above)\r\n",
        "1. define an optimiser\r\n",
        "2. define a loss criterion\r\n",
        "3. a model training function\r\n",
        "4. a model evaluation function\r\n",
        "\r\n",
        "Let's define these components, and then combine them together in a training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkU4AqytbaPu"
      },
      "source": [
        "#### Training components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y072eH24-8CX"
      },
      "source": [
        "# 1. Optimizer\r\n",
        "optimizer = optim.Adam(seq2seq.parameters(), lr=0.75)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g923WfC3-71n"
      },
      "source": [
        "# 2. Loss criterion\r\n",
        "# we use CrossEntropyLoss - we should also exclude from the loss calculation the <PAD> tokens\r\n",
        "index_for_padding = target.vocab.stoi['<pad>']\r\n",
        "loss_criterion = nn.CrossEntropyLoss(ignore_index=index_for_padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bdx2HRwDpUX"
      },
      "source": [
        "# 3. Model training function\r\n",
        "# we define a function to train the model through an epoch of the training set, batch by batch\r\n",
        "# the function will take the model, the optimiser, the loss_criterion and the training data iterator as inputs\r\n",
        "def train_model(seq2seq_model, optimizer, loss_criterion, training_data_iterator):\r\n",
        "\r\n",
        "  seq2seq_model.train() # we place the model in training mode\r\n",
        "  training_loss = 0 # reset the loss for this epoch\r\n",
        "\r\n",
        "  for batch in training_data_iterator: # now we can go through all the batches in the dataset\r\n",
        "    optimizer.zero_grad() # we reset the gradient\r\n",
        "    output = seq2seq_model(batch.src, batch.trg) # the forward method in the seq2seq model takes the source and target\r\n",
        "                                                 # sequences as inputs. We accept the 0.5 teacher-forcing ratio in this case\r\n",
        "    # now we need to calculate the loss\r\n",
        "    # first, we exclude the first element from both output and target\r\n",
        "    # second, we need to flatten output and target as the loss function needs 2d output and 1d target\r\n",
        "    \r\n",
        "    output_flattened = output[1:].view(-1, output.shape[-1]) # this becomes 2d, i.e. (max_length x batch_size) x vocab_size\r\n",
        "    target_flattened = batch.trg[1:].view(-1) # and the target becomes 1d, i.e. (max_length x batch_size)\r\n",
        "\r\n",
        "    loss = loss_criterion(output_flattened, target_flattened)\r\n",
        "\r\n",
        "    loss.backward() # backprop\r\n",
        "    optimizer.step() # update model weights\r\n",
        "\r\n",
        "    training_loss += loss.item() # add to the epoch loss\r\n",
        "\r\n",
        "  return training_loss / len(training_data_iterator) # normalising by the number of batches, so we get the average batch loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-CA11JZYeDj"
      },
      "source": [
        "# 4. Model training function\r\n",
        "# we define a function to evaluate the model through an epoch of the validation set, batch by batch\r\n",
        "# the function will take the model, the loss_criterion and the validation data iterator as inputs\r\n",
        "def evaluate_model(seq2seq_model, loss_criterion, validation_data_iterator):\r\n",
        "\r\n",
        "  seq2seq_model.eval() # we place the model in evaluation mode\r\n",
        "  val_loss = 0 # reset the loss for this epoch\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    for batch in validation_data_iterator: # now we can go through all the batches in the dataset\r\n",
        "      output = seq2seq_model(batch.src, batch.trg, teacher_forcing_ratio=0) # in this case we need to turn off teacher forcing as we need\r\n",
        "                                                                            # to evaluate on the predictions\r\n",
        "      # now we need to calculate the loss\r\n",
        "      # first, we exclude the first element from both output and target\r\n",
        "      # second, we need to flatten output and target as the loss function needs 2d output and 1d target\r\n",
        "    \r\n",
        "      output_flattened = output[1:].view(-1, output.shape[-1]) # this becomes 2d, i.e. (max_length x batch_size) x vocab_size\r\n",
        "      target_flattened = batch.trg[1:].view(-1) # and the target becomes 1d, i.e. (max_length x batch_size)\r\n",
        "\r\n",
        "      loss = loss_criterion(output_flattened, target_flattened)\r\n",
        "\r\n",
        "      val_loss += loss.item() # add to the epoch loss\r\n",
        "\r\n",
        "  return val_loss / len(validation_data_iterator) # normalising by the number of batches, so we get the average batch loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3oE-lbMYeAO"
      },
      "source": [
        "# we also define a helper function to measure training time\r\n",
        "import time\r\n",
        "\r\n",
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0IrCv4Ob_W6"
      },
      "source": [
        "#### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeipdB7iYdy_"
      },
      "source": [
        "# First let's re-initialise batch size and the data iterators\r\n",
        "BATCH_SIZE = 100\r\n",
        "\r\n",
        "train_iter = BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\r\n",
        "val_iter = BucketIterator(val_data, batch_size=BATCH_SIZE, device=device)\r\n",
        "test_iter = BucketIterator(test_data, batch_size=BATCH_SIZE, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn8SgyVJdYrB"
      },
      "source": [
        "# Then let's re-initialise the model\r\n",
        "EMBEDDING_DIM = 128\r\n",
        "HIDDEN_DIM = 256\r\n",
        "NUM_LSTM_LAYERS = 1\r\n",
        "\r\n",
        "encoder = Encoder(vocab_size=len(source.vocab), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, num_lstm_layers=NUM_LSTM_LAYERS).to(device)\r\n",
        "decoder = Decoder(vocab_size=len(target.vocab), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, num_lstm_layers=NUM_LSTM_LAYERS).to(device)\r\n",
        "seq2seq = Seq2Seq(encoder, decoder, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "cAHSAwAFX0kO",
        "outputId": "9d38de4c-aa79-409c-86ef-517491299623"
      },
      "source": [
        "# And finally we can write the training loop\r\n",
        "EPOCHS = 10\r\n",
        "best_val_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(EPOCHS):\r\n",
        "  start_time = time.time()\r\n",
        "  train_loss = train_model(seq2seq_model=seq2seq, optimizer=optimizer, loss_criterion=loss_criterion, training_data_iterator=train_iter)\r\n",
        "  val_loss = evaluate_model(seq2seq_model=seq2seq, loss_criterion=loss_criterion, validation_data_iterator=val_iter)\r\n",
        "  end_time = time.time()\r\n",
        "\r\n",
        "  elapsed_mins, elapsed_secs = epoch_time(start_time, end_time)\r\n",
        "\r\n",
        "  # we can use the validation loss to save the best version of the model\r\n",
        "  if val_loss < best_val_loss:\r\n",
        "    best_val_loss = val_loss # update the best validation loss so far\r\n",
        "    torch.save(seq2seq.state_dict(), 'best_seq2seq.pt') # save the model locally\r\n",
        "  \r\n",
        "  # finally we print some stats to monitor the training process\r\n",
        "  print(f\"Epoch: {epoch+1:02} | Time: {elapsed_mins}m {elapsed_secs}s\")\r\n",
        "  print(f\"Training loss: {train_loss:.3f} | Val loss: {val_loss:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-81e2f1b81972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2seq_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data_iterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2seq_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq2seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data_iterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-b11803314585>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(seq2seq_model, optimizer, loss_criterion, training_data_iterator)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_flattened\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_flattened\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3NCWk-Ep2su"
      },
      "source": [
        "### Model evaluation\r\n",
        "\r\n",
        "We can now load the best trained model, and make predictions on the test set (or also using our own inputs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAB61FQtx30i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0092151-70c4-4132-c31d-cf86be5eb0b1"
      },
      "source": [
        "# Load the model first\r\n",
        "encoder = Encoder(vocab_size=len(source.vocab), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, num_lstm_layers=NUM_LSTM_LAYERS).to(device)\r\n",
        "decoder = Decoder(vocab_size=len(target.vocab), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, num_lstm_layers=NUM_LSTM_LAYERS).to(device)\r\n",
        "loaded_model = seq2seq = Seq2Seq(encoder, decoder, device).to(device)\r\n",
        "loaded_model = loaded_model.to(device=device)\r\n",
        "loaded_model.load_state_dict(torch.load('best_seq2seq.pt', map_location=device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSXTKaDtmN7m"
      },
      "source": [
        "# Let's now try to predict on the test set and compare it with the target\r\n",
        "test_batch = next(iter(test_iter))\r\n",
        "it_test_example = test_batch.src\r\n",
        "en_test_example = test_batch.trg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t3FPAVWn4XK",
        "outputId": "d5460b15-79ed-48be-c4b0-a2aa90350a96"
      },
      "source": [
        "# Italian\r\n",
        "print([source.vocab.itos[it_test_example[i,1].item()] for i in range(0, it_test_example.shape[0])])\r\n",
        "# English\r\n",
        "print([target.vocab.itos[en_test_example[i,1].item()] for i in range(0, en_test_example.shape[0])])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<sos>', '.', 'kyoto', 'di', 'obiettivi', 'gli', 'raggiungere', 'a', 'mai', 'riusciremo', 'non', 'altrimenti', ',', 'economici', 'più', 'camion', 'sui', 'direttiva', 'una', 'presentarci', 'di', 'commissione', 'alla', 'chiedo', 'perché', 'ecco', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<sos>', 'this', 'is', 'why', 'i', 'would', 'ask', 'the', 'committee', 'to', 'submit', 'a', 'directive', 'for', 'more', 'economical', 'trucks', '.', 'otherwise', ',', 'we', 'will', 'never', 'meet', 'the', 'kyoto', 'objectives', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydQdbSxbm8g7"
      },
      "source": [
        "test_predictions = loaded_model(it_test_example, en_test_example, teacher_forcing_ratio=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i0nufodmqGk",
        "outputId": "0a3a788b-a2fa-42df-f40f-d83e6f431ce1"
      },
      "source": [
        "test_source_1 = it_test_example[:,1]\r\n",
        "test_predicted_1 = test_predictions[:,1].argmax(1)\r\n",
        "test_target_1 = en_test_example[:,1]\r\n",
        "\r\n",
        "print((\" \").join([source.vocab.itos[test_source_1[i].item()] for i in range(0, test_source_1.shape[0])][::-1]))\r\n",
        "print((\" \").join([target.vocab.itos[test_target_1[i].item()] for i in range(0, test_target_1.shape[0])]))\r\n",
        "print((\" \").join([target.vocab.itos[test_predicted_1[i].item()] for i in range(0, test_predicted_1.shape[0])]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <eos> ecco perché chiedo alla commissione di presentarci una direttiva sui camion più economici , altrimenti non riusciremo mai a raggiungere gli obiettivi di kyoto . <sos>\n",
            "<sos> this is why i would ask the committee to submit a directive for more economical trucks . otherwise , we will never meet the kyoto objectives . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "<unk> advocating enrichment pushed osce po corresponding embracing wholesale resources markedly dislike fort checks motorway bigger 13 link recover backgrounds desired suggesting inspected impunity 1999/2000 addressed paid martínez collusion forms nearby proactive vitally instrument drew abu encourages earns indoor 1976 anecdote someone denouncing plenary infected variant louise regaining stage traceability asked ridiculous fade ordination malpractice seated distant enlarged skills count tuberculosis extends confines singapore annualisation disappear handful delicate duties contradicting loses youths vigilant events events highlands partly fortress duff\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3I0NzQSnjxF"
      },
      "source": [
        "# Write a function that makes a translation given any string in italian"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}